{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Build a predictive model to estimate a socio-economic status characteristic.\n",
    "Key Deliverables:\n",
    "- Predictive model results.\n",
    "- A comparison of at least two machine learning algorithms.\n",
    "- Predictions on the test set in the specified format.\n",
    "- A reproducible, high-quality report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import impute, experimental\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "    # Learning data\n",
    "learn_df = pd.read_csv(r'learn_dataset.csv')\n",
    "learn_df_job = pd.read_csv(r'learn_dataset_job.csv')\n",
    "learn_df_emp = pd.read_csv(r'learn_dataset_Emp.csv')\n",
    "learn_df_sport = pd.read_csv(r'learn_dataset_sport.csv')\n",
    "learn_df_ret_former = pd.read_csv(r'learn_dataset_retired_former.csv')\n",
    "learn_df_sport_ret_job = pd.read_csv(r'learn_dataset_retired_jobs.csv')\n",
    "learn_df_sport_ret_pension = pd.read_csv(r'learn_dataset_retired_pension.csv')\n",
    "\n",
    "    # Test data\n",
    "test_df = pd.read_csv(r'test_dataset.csv')\n",
    "test_df_job = pd.read_csv(r'test_dataset_job.csv')\n",
    "test_df_emp = pd.read_csv(r'test_dataset_Emp.csv')\n",
    "test_df_sport = pd.read_csv(r'test_dataset_sport.csv')\n",
    "test_df_ret_former = pd.read_csv(r'test_dataset_retired_former.csv')\n",
    "test_df_sport_ret_job = pd.read_csv(r'test_dataset_retired_jobs.csv')\n",
    "test_df_sport_ret_pension = pd.read_csv(r'test_dataset_retired_pension.csv')\n",
    "\n",
    "    # Mapping data\n",
    "# To load au fur et à mesure des besoins\n",
    "\n",
    "# With PRIMARY_KEY used to link datasets - the Identifier \n",
    "# Merging all\n",
    "list_learn = [\n",
    "    'learn_df', 'learn_df_job', 'learn_df_emp', 'learn_df_sport',\n",
    "    'learn_df_ret_former', 'learn_df_sport_ret_job', 'learn_df_sport_ret_pension'\n",
    "]\n",
    "\n",
    "list_test = [\n",
    "    'test_df', 'test_df_job', 'test_df_emp', 'test_df_sport',\n",
    "    'test_df_ret_former', 'test_df_sport_ret_job', 'test_df_sport_ret_pension'\n",
    "]\n",
    "\n",
    "# building the dictionary for the function (to merge all)\n",
    "learn_dic = {name: globals()[name] for name in list_learn}\n",
    "test_dic = {name: globals()[name] for name in list_test}\n",
    "\n",
    "def merge_dfs(datasets_dic, key_column, merge_type=\"left\"):\n",
    "    # Load the first dataset to initialize main_df\n",
    "    main_df = list(datasets_dic.values())[0]\n",
    "\n",
    "    # Iterate over the remaining datasets and merge them\n",
    "    for dataset in list(datasets_dic.values())[1:]:\n",
    "        main_df = main_df.merge(dataset, on=key_column, how=merge_type)\n",
    "    \n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_merged_data = merge_dfs(learn_dic, key_column=\"PRIMARY_KEY\") # no missings on the target column\n",
    "test_merged_data = merge_dfs(test_dic, key_column=\"PRIMARY_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PRIMARY_KEY', 'insee', 'Occupation_42', 'Age_2018', 'IS_STUDENT',\n",
       "       'highest_diploma', 'Act', 'FAMILY_TYPE', 'SEX', 'WORKING_HOURS_x',\n",
       "       'EMPLOYER_CATEGORY_x', 'WAGES', 'JOB_DESCRIPTION_x', 'JOB_CATEGORY_x',\n",
       "       'CONTRACT_TYPE_x', 'JOB_DEP_x', 'Job_condition_x', 'activity_sector_x',\n",
       "       'Employee_count_x', 'Emp', 'club', 'Former_occupation_42', 'Former_emp',\n",
       "       'retirement_age', 'activity_sector_y', 'Former_dep', 'JOB_DEP_y',\n",
       "       'JOB_DESCRIPTION_y', 'CONTRACT_TYPE_y', 'JOB_CATEGORY_y',\n",
       "       'Job_condition_y', 'EMPLOYER_CATEGORY_y', 'WORKING_HOURS_y',\n",
       "       'Employee_count_y', 'pension_plan_payments'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_merged_data.columns\n",
    "test_merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRIMARY_KEY</th>\n",
       "      <th>Age_2018</th>\n",
       "      <th>WORKING_HOURS_x</th>\n",
       "      <th>WAGES</th>\n",
       "      <th>retirement_age</th>\n",
       "      <th>WORKING_HOURS_y</th>\n",
       "      <th>pension_plan_payments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50043.000000</td>\n",
       "      <td>50043.000000</td>\n",
       "      <td>19339.000000</td>\n",
       "      <td>19352.000000</td>\n",
       "      <td>13019.000000</td>\n",
       "      <td>10994.000000</td>\n",
       "      <td>11003.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50081.387087</td>\n",
       "      <td>49.420578</td>\n",
       "      <td>1480.866074</td>\n",
       "      <td>22688.625775</td>\n",
       "      <td>60.323681</td>\n",
       "      <td>1345.436329</td>\n",
       "      <td>18582.447423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28898.102717</td>\n",
       "      <td>20.624439</td>\n",
       "      <td>540.123814</td>\n",
       "      <td>14133.362614</td>\n",
       "      <td>2.873665</td>\n",
       "      <td>598.269351</td>\n",
       "      <td>7828.204575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>8881.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25108.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1211.000000</td>\n",
       "      <td>13198.500000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>892.000000</td>\n",
       "      <td>13163.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50071.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1676.000000</td>\n",
       "      <td>20484.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1517.000000</td>\n",
       "      <td>16987.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>75024.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1820.000000</td>\n",
       "      <td>28970.500000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>1820.000000</td>\n",
       "      <td>21834.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100084.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>146523.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>149194.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PRIMARY_KEY      Age_2018  WORKING_HOURS_x          WAGES  \\\n",
       "count   50043.000000  50043.000000     19339.000000   19352.000000   \n",
       "mean    50081.387087     49.420578      1480.866074   22688.625775   \n",
       "std     28898.102717     20.624439       540.123814   14133.362614   \n",
       "min         2.000000     15.000000         7.000000     314.000000   \n",
       "25%     25108.500000     32.000000      1211.000000   13198.500000   \n",
       "50%     50071.000000     49.000000      1676.000000   20484.000000   \n",
       "75%     75024.000000     65.000000      1820.000000   28970.500000   \n",
       "max    100084.000000    119.000000      3000.000000  146523.000000   \n",
       "\n",
       "       retirement_age  WORKING_HOURS_y  pension_plan_payments  \n",
       "count    13019.000000     10994.000000           11003.000000  \n",
       "mean        60.323681      1345.436329           18582.447423  \n",
       "std          2.873665       598.269351            7828.204575  \n",
       "min         34.000000        34.000000            8881.000000  \n",
       "25%         60.000000       892.000000           13163.000000  \n",
       "50%         60.000000      1517.000000           16987.000000  \n",
       "75%         62.000000      1820.000000           21834.000000  \n",
       "max         70.000000      3000.000000          149194.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = learn_merged_data.describe() # for numerical values\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PRIMARY_KEY               0.000000\n",
       "insee                     0.000000\n",
       "Occupation_42             0.000000\n",
       "Age_2018                  0.000000\n",
       "IS_STUDENT                0.000000\n",
       "highest_diploma           0.000000\n",
       "Act                       0.000000\n",
       "FAMILY_TYPE               0.000000\n",
       "SEX                       0.000000\n",
       "target                    0.000000\n",
       "WORKING_HOURS_x          61.355234\n",
       "EMPLOYER_CATEGORY_x      62.596167\n",
       "WAGES                    61.329257\n",
       "JOB_DESCRIPTION_x        61.329257\n",
       "JOB_CATEGORY_x           61.329257\n",
       "CONTRACT_TYPE_x          61.329257\n",
       "JOB_DEP_x                61.377216\n",
       "Job_condition_x          61.329257\n",
       "activity_sector_x        61.329257\n",
       "Employee_count_x         62.394341\n",
       "Emp                      51.641588\n",
       "club                     87.069121\n",
       "Former_occupation_42     73.984373\n",
       "Former_emp               73.984373\n",
       "retirement_age           73.984373\n",
       "activity_sector_y        78.012909\n",
       "Former_dep               78.700318\n",
       "JOB_DEP_y                78.662350\n",
       "JOB_DESCRIPTION_y        78.012909\n",
       "CONTRACT_TYPE_y          78.012909\n",
       "JOB_CATEGORY_y           78.012909\n",
       "Job_condition_y          78.012909\n",
       "EMPLOYER_CATEGORY_y      79.515617\n",
       "WORKING_HOURS_y          78.030893\n",
       "Employee_count_y         79.353756\n",
       "pension_plan_payments    78.012909\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_rate = learn_merged_data.isnull().mean() * 100\n",
    "missing_rate # wesh that's a lot of missings RIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing - CLEANING + handling the missings and unsure variables are formatted corectly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns for retirement \n",
    "learn_merged_data.rename(columns={'JOB_DESCRIPTION_x': 'JOB_DESCRIPTION', \n",
    "                                'WORKING_HOURS_x': 'WORKING_HOURS',\n",
    "                                'EMPLOYER_CATEGORY_x': 'EMPLOYER_CATEGORY',\n",
    "                                'JOB_CATEGORY_x': 'JOB_CATEGORY',\n",
    "                                'CONTRACT_TYPE_x': 'CONTRACT_TYPE',\n",
    "                                'JOB_DEP_x': 'JOB_DEP',\n",
    "                                'activity_sector_x': 'activity_sector',\n",
    "                                'Employee_count_x': 'Employee_count',\n",
    "                                'activity_sector_y': 'ret_activity_sector',\n",
    "                                'JOB_DEP_y': 'ret_JOB_DEP',\n",
    "                                'JOB_DESCRIPTION_y': 'ret_JOB_DESCRIPTION',\n",
    "                                'CONTRACT_TYPE_y': 'ret_CONTRACT_TYPE',\n",
    "                                'JOB_CATEGORY_y': 'ret_JOB_CATEGORY',\n",
    "                                'Job_condition_y': 'ret_Job_condition',\n",
    "                                'EMPLOYER_CATEGORY_y': 'ret_EMPLOYER_CATEGORY',\n",
    "                                'WORKING_HOURS_y': 'ret_WORKING_HOURS',\n",
    "                                'Employee_count_y': 'ret_Employee_count'}, inplace=True)\n",
    "\n",
    "\n",
    "# clean a bit and dropping coluumns - If a variable has too much missing data (e.g., >50%)\n",
    "# # we drop it, especially if it isn't critical to the target variable.\n",
    "learn_merged_data.drop(columns=['Job_condition_x', \n",
    "                                'ret_Job_condition',\n",
    "                                'JOB_DESCRIPTION', \n",
    "                                'ret_JOB_DESCRIPTION', \n",
    "                                'JOB_CATEGORY', \n",
    "                                'ret_JOB_CATEGORY',\n",
    "                                'Employee_count',\n",
    "                                'ret_Employee_count'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50043\n",
      "50043\n",
      "10595\n"
     ]
    }
   ],
   "source": [
    "    # Missings - sachant qu'ils ne faut pas les drop du test set: \"observations with missing data cannot be removed from the test set\"\n",
    "# c'est important car: \n",
    "# Handling cases where some information is entirely missing for a given person \n",
    "# permettra de ensure that our model can still make meaningful predictions \n",
    "# without introducing bias or inaccuracies.\n",
    "   \n",
    "    # isolons les rows with missings \n",
    "\n",
    "print(learn_merged_data.shape[0])\n",
    "\n",
    "df_na_only = learn_merged_data[learn_merged_data.isna().any(axis=1)]\n",
    "print(df_na_only.shape[0]) # all individuals have at least one missing value \n",
    "\n",
    "df_na_only['NaN_count'] = df_na_only.isna().sum(axis=1) # max sum NA = 18 \n",
    "df_max_na = df_na_only[df_na_only['NaN_count'] == 18]\n",
    "print(df_max_na.shape[0]) # 10 595 individuals where : information entirely missing (outside the initial info coming from the learn_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONE BUT TO CHECK AGAIN : on pourrait fill les missings selon le type de la variable \n",
    "Numerical variables - on fill en utilisant the patterns in other variables to estimate missing values.\n",
    "    # Example: Iterative Imputation with multiple models\n",
    "\n",
    "    # DOCUMENTATION: \n",
    "The IterativeImputer in sklearn is a great tool for this, as it models each feature with missing values \n",
    "as a function of other features and iteratively imputes missing values.\n",
    "\n",
    "    # Initialize IterativeImputer with different estimators (models)\n",
    "You can specify different models for each feature using the 'estimator' parameter\n",
    "\n",
    "Using LinearRegression and RandomForestRegressor as estimators\n",
    "LinearRegression for imputation of continuous features\n",
    "RandomForestRegressor can be used to model non-linear relationships\n",
    "\n",
    "Note: You can use any estimator that works for your data (regression models, decision trees, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure columns are the right type\n",
    "learn_merged_data.dtypes # corriger ici les variables non reconnues comme numériques\n",
    "\n",
    "col_convert = [\"ret_JOB_DEP\", \"Former_dep\", \"JOB_DEP\"]\n",
    "\n",
    "# Convert specified columns to float\n",
    "learn_merged_data[col_convert] = learn_merged_data[col_convert].apply(pd.to_numeric, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/impute/_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "iterative_imputer = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(n_estimators=10, random_state=42),\n",
    "    max_iter=10,  # Maximum number of iterations\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# this method only handles numerical values\n",
    "numerical_cols = learn_merged_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Apply the iterative imputer to fill missing values\n",
    "# This will fill missing values using patterns in other variables\n",
    "# df_imputed = pd.DataFrame(iterative_imputer.fit_transform(learn_merged_data), columns=learn_merged_data.columns)\n",
    "learn_merged_data[numerical_cols] = iterative_imputer.fit_transform(learn_merged_data[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rz/f98wb9gj7vz0bnm2ntxmyv3m0000gn/T/ipykernel_56662/783668530.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  learn_merged_data[col].fillna('unknown', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Categorical variables - replace missings with \"unkonwn\" \n",
    "# This allows the model to treat missing values as a separate category \n",
    "# rather than just ignoring them or replacing them with the most common value.\n",
    "categorical_cols = learn_merged_data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    learn_merged_data[col].fillna('unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PRIMARY_KEY              0.0\n",
       "insee                    0.0\n",
       "Occupation_42            0.0\n",
       "Age_2018                 0.0\n",
       "IS_STUDENT               0.0\n",
       "highest_diploma          0.0\n",
       "Act                      0.0\n",
       "FAMILY_TYPE              0.0\n",
       "SEX                      0.0\n",
       "target                   0.0\n",
       "WORKING_HOURS            0.0\n",
       "EMPLOYER_CATEGORY        0.0\n",
       "WAGES                    0.0\n",
       "CONTRACT_TYPE            0.0\n",
       "JOB_DEP                  0.0\n",
       "activity_sector          0.0\n",
       "Emp                      0.0\n",
       "club                     0.0\n",
       "Former_occupation_42     0.0\n",
       "Former_emp               0.0\n",
       "retirement_age           0.0\n",
       "ret_activity_sector      0.0\n",
       "Former_dep               0.0\n",
       "ret_JOB_DEP              0.0\n",
       "ret_CONTRACT_TYPE        0.0\n",
       "ret_EMPLOYER_CATEGORY    0.0\n",
       "ret_WORKING_HOURS        0.0\n",
       "pension_plan_payments    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_rate = learn_merged_data.isnull().mean() * 100\n",
    "missing_rate # OK à vérifier si la méthode et la cohérence des values qui ont été fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : Should we reduce the dataset to keep a predictive power ? Genre filtrer que sur certaines villes/sur une tranche d'âges ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation for numerical variables only \n",
    "# Convert the target variable into a binary variable pour l'avoir sur le matrix\n",
    "# learn_merged_data['target'] = learn_merged_data['target'].replace({'V': 1, 'A': 2})\n",
    "# numerical_data = learn_merged_data.select_dtypes(include=['number'])\n",
    "\n",
    "# # correlation matrix\n",
    "# correlation_matrix = numerical_data.corr()\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt='.2f')\n",
    "# plt.title('Correlation Matrix')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build predictive models - generate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and optimize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
