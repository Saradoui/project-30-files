{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Build a predictive model to estimate a socio-economic status characteristic.\n",
    "Key Deliverables:\n",
    "- Predictive model results.\n",
    "- A comparison of at least two machine learning algorithms.\n",
    "- Predictions on the test set in the specified format.\n",
    "- A reproducible, high-quality report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m impute, experimental\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_iterative_imputer  \n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IterativeImputer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# IMPORTS \n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import impute, experimental\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "    # Learning data\n",
    "learn_df = pd.read_csv(r'learn_dataset.csv')\n",
    "learn_df_job = pd.read_csv(r'learn_dataset_job.csv')\n",
    "learn_df_emp = pd.read_csv(r'learn_dataset_Emp.csv')\n",
    "learn_df_sport = pd.read_csv(r'learn_dataset_sport.csv')\n",
    "learn_df_ret_former = pd.read_csv(r'learn_dataset_retired_former.csv')\n",
    "learn_df_sport_ret_job = pd.read_csv(r'learn_dataset_retired_jobs.csv')\n",
    "learn_df_sport_ret_pension = pd.read_csv(r'learn_dataset_retired_pension.csv')\n",
    "\n",
    "    # Test data\n",
    "test_df = pd.read_csv(r'test_dataset.csv')\n",
    "test_df_job = pd.read_csv(r'test_dataset_job.csv')\n",
    "test_df_emp = pd.read_csv(r'test_dataset_Emp.csv')\n",
    "test_df_sport = pd.read_csv(r'test_dataset_sport.csv')\n",
    "test_df_ret_former = pd.read_csv(r'test_dataset_retired_former.csv')\n",
    "test_df_sport_ret_job = pd.read_csv(r'test_dataset_retired_jobs.csv')\n",
    "test_df_sport_ret_pension = pd.read_csv(r'test_dataset_retired_pension.csv')\n",
    "\n",
    "    # Mapping data\n",
    "# To load au fur et à mesure des besoins\n",
    "\n",
    "# With PRIMARY_KEY used to link datasets - the Identifier \n",
    "# Merging all\n",
    "list_learn = [\n",
    "    'learn_df', 'learn_df_job', 'learn_df_emp', 'learn_df_sport',\n",
    "    'learn_df_ret_former', 'learn_df_sport_ret_job', 'learn_df_sport_ret_pension'\n",
    "]\n",
    "\n",
    "list_test = [\n",
    "    'test_df', 'test_df_job', 'test_df_emp', 'test_df_sport',\n",
    "    'test_df_ret_former', 'test_df_sport_ret_job', 'test_df_sport_ret_pension'\n",
    "]\n",
    "\n",
    "# building the dictionary for the function (to merge all)\n",
    "learn_dic = {name: globals()[name] for name in list_learn}\n",
    "test_dic = {name: globals()[name] for name in list_test}\n",
    "\n",
    "def merge_dfs(datasets_dic, key_column, merge_type=\"left\"):\n",
    "    # Load the first dataset to initialize main_df\n",
    "    main_df = list(datasets_dic.values())[0]\n",
    "\n",
    "    # Iterate over the remaining datasets and merge them\n",
    "    for dataset in list(datasets_dic.values())[1:]:\n",
    "        main_df = main_df.merge(dataset, on=key_column, how=merge_type)\n",
    "    \n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_merged_data = merge_dfs(learn_dic, key_column=\"PRIMARY_KEY\") # no missings on the target column\n",
    "test_merged_data = merge_dfs(test_dic, key_column=\"PRIMARY_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PRIMARY_KEY', 'insee', 'Occupation_42', 'Age_2018', 'IS_STUDENT',\n",
       "       'highest_diploma', 'Act', 'FAMILY_TYPE', 'SEX', 'WORKING_HOURS_x',\n",
       "       'EMPLOYER_CATEGORY_x', 'WAGES', 'JOB_DESCRIPTION_x', 'JOB_CATEGORY_x',\n",
       "       'CONTRACT_TYPE_x', 'JOB_DEP_x', 'Job_condition_x', 'activity_sector_x',\n",
       "       'Employee_count_x', 'Emp', 'club', 'Former_occupation_42', 'Former_emp',\n",
       "       'retirement_age', 'activity_sector_y', 'Former_dep', 'JOB_DEP_y',\n",
       "       'JOB_DESCRIPTION_y', 'CONTRACT_TYPE_y', 'JOB_CATEGORY_y',\n",
       "       'Job_condition_y', 'EMPLOYER_CATEGORY_y', 'WORKING_HOURS_y',\n",
       "       'Employee_count_y', 'pension_plan_payments'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_merged_data.columns\n",
    "test_merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRIMARY_KEY</th>\n",
       "      <th>Age_2018</th>\n",
       "      <th>WORKING_HOURS</th>\n",
       "      <th>WAGES</th>\n",
       "      <th>retirement_age</th>\n",
       "      <th>ret_WORKING_HOURS</th>\n",
       "      <th>pension_plan_payments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50043.000000</td>\n",
       "      <td>50043.000000</td>\n",
       "      <td>19339.000000</td>\n",
       "      <td>19352.000000</td>\n",
       "      <td>13019.000000</td>\n",
       "      <td>10994.000000</td>\n",
       "      <td>11003.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50081.387087</td>\n",
       "      <td>49.420578</td>\n",
       "      <td>1480.866074</td>\n",
       "      <td>22688.625775</td>\n",
       "      <td>60.323681</td>\n",
       "      <td>1345.436329</td>\n",
       "      <td>18582.447423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28898.102717</td>\n",
       "      <td>20.624439</td>\n",
       "      <td>540.123814</td>\n",
       "      <td>14133.362614</td>\n",
       "      <td>2.873665</td>\n",
       "      <td>598.269351</td>\n",
       "      <td>7828.204575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>8881.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25108.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1211.000000</td>\n",
       "      <td>13198.500000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>892.000000</td>\n",
       "      <td>13163.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50071.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1676.000000</td>\n",
       "      <td>20484.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1517.000000</td>\n",
       "      <td>16987.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>75024.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1820.000000</td>\n",
       "      <td>28970.500000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>1820.000000</td>\n",
       "      <td>21834.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100084.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>146523.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>149194.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PRIMARY_KEY      Age_2018  WORKING_HOURS          WAGES  \\\n",
       "count   50043.000000  50043.000000   19339.000000   19352.000000   \n",
       "mean    50081.387087     49.420578    1480.866074   22688.625775   \n",
       "std     28898.102717     20.624439     540.123814   14133.362614   \n",
       "min         2.000000     15.000000       7.000000     314.000000   \n",
       "25%     25108.500000     32.000000    1211.000000   13198.500000   \n",
       "50%     50071.000000     49.000000    1676.000000   20484.000000   \n",
       "75%     75024.000000     65.000000    1820.000000   28970.500000   \n",
       "max    100084.000000    119.000000    3000.000000  146523.000000   \n",
       "\n",
       "       retirement_age  ret_WORKING_HOURS  pension_plan_payments  \n",
       "count    13019.000000       10994.000000           11003.000000  \n",
       "mean        60.323681        1345.436329           18582.447423  \n",
       "std          2.873665         598.269351            7828.204575  \n",
       "min         34.000000          34.000000            8881.000000  \n",
       "25%         60.000000         892.000000           13163.000000  \n",
       "50%         60.000000        1517.000000           16987.000000  \n",
       "75%         62.000000        1820.000000           21834.000000  \n",
       "max         70.000000        3000.000000          149194.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = learn_merged_data.describe() # for numerical values\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PRIMARY_KEY               0.000000\n",
       "insee                     0.000000\n",
       "Occupation_42             0.000000\n",
       "Age_2018                  0.000000\n",
       "IS_STUDENT                0.000000\n",
       "highest_diploma           0.000000\n",
       "Act                       0.000000\n",
       "FAMILY_TYPE               0.000000\n",
       "SEX                       0.000000\n",
       "target                    0.000000\n",
       "WORKING_HOURS            61.355234\n",
       "EMPLOYER_CATEGORY        62.596167\n",
       "WAGES                    61.329257\n",
       "CONTRACT_TYPE            61.329257\n",
       "JOB_DEP                  61.377216\n",
       "activity_sector          61.329257\n",
       "Emp                      51.641588\n",
       "club                     87.069121\n",
       "Former_occupation_42     73.984373\n",
       "Former_emp               73.984373\n",
       "retirement_age           73.984373\n",
       "ret_activity_sector      78.012909\n",
       "Former_dep               78.700318\n",
       "ret_JOB_DEP              78.662350\n",
       "ret_CONTRACT_TYPE        78.012909\n",
       "ret_EMPLOYER_CATEGORY    79.515617\n",
       "ret_WORKING_HOURS        78.030893\n",
       "pension_plan_payments    78.012909\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_rate = learn_merged_data.isnull().mean() * 100\n",
    "missing_rate # wesh that's a lot of missings RIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing - CLEANING + handling the missings and unsure variables are formatted corectly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns for retirement \n",
    "learn_merged_data.rename(columns={'JOB_DESCRIPTION_x': 'JOB_DESCRIPTION', \n",
    "                                'WORKING_HOURS_x': 'WORKING_HOURS',\n",
    "                                'EMPLOYER_CATEGORY_x': 'EMPLOYER_CATEGORY',\n",
    "                                'JOB_CATEGORY_x': 'JOB_CATEGORY',\n",
    "                                'CONTRACT_TYPE_x': 'CONTRACT_TYPE',\n",
    "                                'JOB_DEP_x': 'JOB_DEP',\n",
    "                                'activity_sector_x': 'activity_sector',\n",
    "                                'Employee_count_x': 'Employee_count',\n",
    "                                'activity_sector_y': 'ret_activity_sector',\n",
    "                                'JOB_DEP_y': 'ret_JOB_DEP',\n",
    "                                'JOB_DESCRIPTION_y': 'ret_JOB_DESCRIPTION',\n",
    "                                'CONTRACT_TYPE_y': 'ret_CONTRACT_TYPE',\n",
    "                                'JOB_CATEGORY_y': 'ret_JOB_CATEGORY',\n",
    "                                'Job_condition_y': 'ret_Job_condition',\n",
    "                                'EMPLOYER_CATEGORY_y': 'ret_EMPLOYER_CATEGORY',\n",
    "                                'WORKING_HOURS_y': 'ret_WORKING_HOURS',\n",
    "                                'Employee_count_y': 'ret_Employee_count'}, inplace=True)\n",
    "\n",
    "\n",
    "# clean a bit and dropping coluumns - If a variable has too much missing data (e.g., >50%)\n",
    "# # we drop it, especially if it isn't critical to the target variable.\n",
    "learn_merged_data.drop(columns=['Job_condition_x', \n",
    "                                'ret_Job_condition',\n",
    "                                'JOB_DESCRIPTION', \n",
    "                                'ret_JOB_DESCRIPTION', \n",
    "                                'JOB_CATEGORY', \n",
    "                                'ret_JOB_CATEGORY',\n",
    "                                'Employee_count',\n",
    "                                'ret_Employee_count'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50043\n",
      "50043\n",
      "10595\n"
     ]
    }
   ],
   "source": [
    "    # Missings - sachant qu'ils ne faut pas les drop du test set: \"observations with missing data cannot be removed from the test set\"\n",
    "# c'est important car: \n",
    "# Handling cases where some information is entirely missing for a given person \n",
    "# permettra de ensure that our model can still make meaningful predictions \n",
    "# without introducing bias or inaccuracies.\n",
    "   \n",
    "    # isolons les rows with missings \n",
    "\n",
    "print(learn_merged_data.shape[0])\n",
    "\n",
    "df_na_only = learn_merged_data[learn_merged_data.isna().any(axis=1)]\n",
    "print(df_na_only.shape[0]) # all individuals have at least one missing value \n",
    "\n",
    "df_na_only['NaN_count'] = df_na_only.isna().sum(axis=1) # max sum NA = 18 \n",
    "df_max_na = df_na_only[df_na_only['NaN_count'] == 18]\n",
    "print(df_max_na.shape[0]) # 10 595 individuals where : information entirely missing (outside the initial info coming from the learn_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONE BUT TO CHECK AGAIN : on pourrait fill les missings selon le type de la variable \n",
    "Numerical variables - on fill en utilisant the patterns in other variables to estimate missing values.\n",
    "    # Example: Iterative Imputation with multiple models\n",
    "\n",
    "    # DOCUMENTATION: \n",
    "The IterativeImputer in sklearn is a great tool for this, as it models each feature with missing values \n",
    "as a function of other features and iteratively imputes missing values.\n",
    "\n",
    "    # Initialize IterativeImputer with different estimators (models)\n",
    "You can specify different models for each feature using the 'estimator' parameter\n",
    "\n",
    "Using LinearRegression and RandomForestRegressor as estimators\n",
    "LinearRegression for imputation of continuous features\n",
    "RandomForestRegressor can be used to model non-linear relationships\n",
    "\n",
    "Note: You can use any estimator that works for your data (regression models, decision trees, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/sklearn/impute/_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "iterative_imputer = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(n_estimators=10, random_state=42),\n",
    "    max_iter=10,  # Maximum number of iterations\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# this method only handles numerical values\n",
    "numerical_cols = learn_merged_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Apply the iterative imputer to fill missing values\n",
    "# This will fill missing values using patterns in other variables\n",
    "# df_imputed = pd.DataFrame(iterative_imputer.fit_transform(learn_merged_data), columns=learn_merged_data.columns)\n",
    "learn_merged_data[numerical_cols] = iterative_imputer.fit_transform(learn_merged_data[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables - replace missings with \"unkonwn\" \n",
    "# This allows the model to treat missing values as a separate category \n",
    "# rather than just ignoring them or replacing them with the most common value.\n",
    "categorical_cols = learn_merged_data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    learn_merged_data[col].fillna('unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rate = learn_merged_data.isnull().mean() * 100\n",
    "missing_rate # OK à vérifier sur la méthode et la cohérence des values qui ont été fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : Should we reduce the dataset to keep a predictive power ? Genre filtrer que sur certaines villes/sur une tranche d'âges ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation for numerical variables\n",
    "correlation_matrix = learn_merged_data.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build predictive models - generate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and optimize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
